{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelMasterr/Machine-Learning-with-Python/blob/main/NPL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgjJjA7nxaL5",
        "outputId": "f1496c6f-0ef2-4f7d-f377-d4cf47c49081"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenización**\n",
        "La tokenización es el proceso de dividir un texto en unidades más pequeñas, llamadas tokens, que pueden ser palabras, subpalabras o caracteres. Estos tokens luego pueden analizarse o procesarse individualmente."
      ],
      "metadata": {
        "id": "PH40TCQkDa-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"El perro corre felizmente por el parque todas las mañanas\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwxAY624CwVW",
        "outputId": "088636db-d1af-4c7f-a1fe-9652fec62bc5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'perro', 'corre', 'felizmente', 'por', 'el', 'parque', 'todas', 'las', 'mañanas']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texts = [\n",
        "    \"El perro corre felizmente por el parque todas las mañanas.\",\n",
        "    \"Ayer estudié para el examen de matemáticas durante tres horas.\",\n",
        "    \"La lluvia cesó justo a tiempo para que pudiéramos salir de casa.\",\n",
        "    \"Ellos prefieren leer libros antes de dormir cada noche.\",\n",
        "    \"María compró un ramo de flores para decorar la sala de estar.\"\n",
        "]\n",
        "\n",
        "tokens = [word_tokenize(text) for text in texts]\n",
        "\n",
        "for token_list in tokens:\n",
        "    print(token_list)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kt0DUsv1HyK",
        "outputId": "eb0627b5-3f4c-434d-d994-393ccf01e324"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'perro', 'corre', 'felizmente', 'por', 'el', 'parque', 'todas', 'las', 'mañanas', '.']\n",
            "['Ayer', 'estudié', 'para', 'el', 'examen', 'de', 'matemáticas', 'durante', 'tres', 'horas', '.']\n",
            "['La', 'lluvia', 'cesó', 'justo', 'a', 'tiempo', 'para', 'que', 'pudiéramos', 'salir', 'de', 'casa', '.']\n",
            "['Ellos', 'prefieren', 'leer', 'libros', 'antes', 'de', 'dormir', 'cada', 'noche', '.']\n",
            "['María', 'compró', 'un', 'ramo', 'de', 'flores', 'para', 'decorar', 'la', 'sala', 'de', 'estar', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lematización y stemming**\n",
        "Stemming reduce las palabras a su raíz o base, eliminando afijos como \"-ing\", \"-es\", etc.\n",
        "\n",
        "Lematización convierte las palabras a su forma base o lema basándose en el contexto, produciendo formas gramaticales correctas."
      ],
      "metadata": {
        "id": "EN6Y_TDdDjnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"easily\"]\n",
        "stems = [ps.stem(word) for word in words]\n",
        "print(stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNAxM0roDl3g",
        "outputId": "6854ace1-0eb2-4fb3-9a04-2577b1c6b98f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'fli', 'easili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"caminando\", \"flies\", \"easily\"]\n",
        "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]  # v para verbos\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D71kcv9FWA-",
        "outputId": "c5b7da3f-d0e0-4028-d71e-06c7a6413d0d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['caminando', 'fly', 'easily']\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "verbs = ['running', 'playing', 'singing', 'dancing', 'walking',\n",
        "         'goes', 'does', 'watches', 'fixes', 'studies',\n",
        "         'eating', 'writing', 'reading', 'speaking', 'coding']\n",
        "\n",
        "lemmas = [lemmatizer.lemmatize(verb, pos='v') for verb in verbs]  # 'v' para verbo\n",
        "\n",
        "for verb, lemma in zip(verbs, lemmas):\n",
        "    print(f'{verb} -> {lemma}')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbLfKOSjChJr",
        "outputId": "376fef8b-8b99-455e-ceaa-10d26d103db0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "playing -> play\n",
            "singing -> sing\n",
            "dancing -> dance\n",
            "walking -> walk\n",
            "goes -> go\n",
            "does -> do\n",
            "watches -> watch\n",
            "fixes -> fix\n",
            "studies -> study\n",
            "eating -> eat\n",
            "writing -> write\n",
            "reading -> read\n",
            "speaking -> speak\n",
            "coding -> cod\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bag of Words (BoW)**\n",
        "Es una representación simple de texto que ignora la gramática y el orden de las palabras. En BoW, el texto se representa como un conjunto de palabras únicas (vocabulario) con su frecuencia."
      ],
      "metadata": {
        "id": "4FN5bQqsFYmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\"El perro corre felizmente por el parque todas las mañanas\", \"This document is the second document\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO3ERMS9FcdZ",
        "outputId": "776e7670-d1ec-44d3-962a-0670b32da2e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['corre' 'document' 'el' 'felizmente' 'is' 'las' 'mañanas' 'parque'\n",
            " 'perro' 'por' 'second' 'the' 'this' 'todas']\n",
            "[[1 0 2 1 0 1 1 1 1 1 0 0 0 1]\n",
            " [0 2 0 0 1 0 0 0 0 0 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\n",
        "    \"El perro corre felizmente por el parque todas las mañanas.\",\n",
        "    \"Ayer estudié para el examen de matemáticas durante tres horas.\",\n",
        "    \"La lluvia cesó justo a tiempo para que pudiéramos salir de casa.\",\n",
        "    \"Ellos prefieren leer libros antes de dormir cada noche.\",\n",
        "    \"María compró un ramo de flores para decorar la sala de estar.\"\n",
        "]\n",
        "# 1. Crear un objeto CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "# 2. Ajustar el vectorizador a los textos para crear el vocabulario\n",
        "vectorizer.fit(texts)\n",
        "# 3. Transformar los textos en vectores BoW\n",
        "bow_vectors = vectorizer.transform(texts)\n",
        "# 4. Imprimir los vectores BoW\n",
        "print(bow_vectors.toarray())\n",
        "\n",
        "# Imprimir el vocabulario, es esencialmente un diccionario donde:\n",
        "# Las claves son las palabras únicas encontradas en tus textos (las oraciones que proporcionaste).\n",
        "# Los valores son los índices numéricos asignados a cada palabra.\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Imprimir una lista ordenada de todas las palabras únicas que CountVectorizer ha identificado en tus textos.\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3rvxKBC20fF",
        "outputId": "169cd53b-f3f4-4e64-e621-7489f3ff04ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0\n",
            "  0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0]\n",
            " [0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1\n",
            "  0 0 1 1 0 0 0]\n",
            " [1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 2 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
            "  1 1 0 0 0 0 1]]\n",
            "{'el': 11, 'perro': 31, 'corre': 6, 'felizmente': 16, 'por': 32, 'parque': 30, 'todas': 40, 'las': 21, 'mañanas': 27, 'ayer': 1, 'estudié': 14, 'para': 29, 'examen': 15, 'de': 7, 'matemáticas': 26, 'durante': 10, 'tres': 41, 'horas': 18, 'la': 20, 'lluvia': 24, 'cesó': 4, 'justo': 19, 'tiempo': 39, 'que': 35, 'pudiéramos': 34, 'salir': 38, 'casa': 3, 'ellos': 12, 'prefieren': 33, 'leer': 22, 'libros': 23, 'antes': 0, 'dormir': 9, 'cada': 2, 'noche': 28, 'maría': 25, 'compró': 5, 'un': 42, 'ramo': 36, 'flores': 17, 'decorar': 8, 'sala': 37, 'estar': 13}\n",
            "['antes' 'ayer' 'cada' 'casa' 'cesó' 'compró' 'corre' 'de' 'decorar'\n",
            " 'dormir' 'durante' 'el' 'ellos' 'estar' 'estudié' 'examen' 'felizmente'\n",
            " 'flores' 'horas' 'justo' 'la' 'las' 'leer' 'libros' 'lluvia' 'maría'\n",
            " 'matemáticas' 'mañanas' 'noche' 'para' 'parque' 'perro' 'por' 'prefieren'\n",
            " 'pudiéramos' 'que' 'ramo' 'sala' 'salir' 'tiempo' 'todas' 'tres' 'un']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "TF-IDF es una medida de relevancia que toma en cuenta cuántas veces aparece una palabra en un documento (frecuencia de término) y en cuántos documentos está presente (frecuencia inversa de documento)."
      ],
      "metadata": {
        "id": "mTz2qHBIFe19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\"This is a sample document\", \"This document is the second document\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Yt5897FisB",
        "outputId": "c7a66db5-d026-4404-fc9c-db5828b2585a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['document' 'is' 'sample' 'second' 'the' 'this']\n",
            "[[0.44832087 0.44832087 0.63009934 0.         0.         0.44832087]\n",
            " [0.63402146 0.31701073 0.         0.44554752 0.44554752 0.31701073]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos de lenguaje: Word2Vec y GloVe**\n",
        "Estos modelos generan representaciones vectoriales para palabras que capturan su significado semántico en función de su contexto.\n",
        "\n",
        "Word2Vec: Genera representaciones vectoriales de palabras usando técnicas como skip-gram o CBOW (continuous bag of words).\n",
        "\n",
        "GloVe: Es un modelo basado en matrices de co-ocurrencia, que busca capturar relaciones semánticas a través de las palabras.\n",
        "\n"
      ],
      "metadata": {
        "id": "jIA3IagDFs1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
        "model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, workers=4)\n",
        "vector = model.wv['dog']  # Vector para la palabra 'cat'\n",
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrxIrZDkFsVJ",
        "outputId": "b3b472f9-9a82-4810-d295-176d5adc6162"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.07311766  0.05070262  0.06757693  0.00762866  0.06350891 -0.03405366\n",
            " -0.00946401  0.05768573 -0.07521638 -0.03936104]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "texts = [\n",
        "    \"El perro corre felizmente por el parque todas las mañanas.\",\n",
        "    \"Ayer estudié para el examen de matemáticas durante tres horas.\",\n",
        "    \"La lluvia cesó justo a tiempo para que pudiéramos salir de casa.\",\n",
        "    \"Ellos prefieren leer libros antes de dormir cada noche.\",\n",
        "    \"María compró un ramo de flores para decorar la sala de estar.\"\n",
        "]\n",
        "\n",
        "# 1. Tokenizar las oraciones\n",
        "tokenized_texts = [word_tokenize(text) for text in texts]\n",
        "print(tokenized_texts)\n",
        "\n",
        "# 2. Entrenar el modelo Word2Vec\n",
        "sentences = tokenized_texts\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# 3. Calcular el vector para cada oración (promedio de vectores de palabras)\n",
        "def get_sentence_vector(sentence, model):\n",
        "    \"\"\"Calcula el vector de una oración promediando los vectores de sus palabras.\"\"\"\n",
        "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if vectors:\n",
        "        return sum(vectors) / len(vectors)\n",
        "    else:\n",
        "        return [0] * model.vector_size  # Vector de ceros si no hay palabras en el vocabulario\n",
        "\n",
        "sentence_vectors = [get_sentence_vector(sentence, model) for sentence in tokenized_texts]\n",
        "\n",
        "# 4. Imprimir los vectores de las oraciones\n",
        "for vector in sentence_vectors:\n",
        "    print(vector)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYgktGmh5GNk",
        "outputId": "3db3db70-20cc-4530-9845-b84c612c9aff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['El', 'perro', 'corre', 'felizmente', 'por', 'el', 'parque', 'todas', 'las', 'mañanas', '.'], ['Ayer', 'estudié', 'para', 'el', 'examen', 'de', 'matemáticas', 'durante', 'tres', 'horas', '.'], ['La', 'lluvia', 'cesó', 'justo', 'a', 'tiempo', 'para', 'que', 'pudiéramos', 'salir', 'de', 'casa', '.'], ['Ellos', 'prefieren', 'leer', 'libros', 'antes', 'de', 'dormir', 'cada', 'noche', '.'], ['María', 'compró', 'un', 'ramo', 'de', 'flores', 'para', 'decorar', 'la', 'sala', 'de', 'estar', '.']]\n",
            "[ 7.2593760e-04  2.5326270e-03  8.6171250e-04  2.7890371e-03\n",
            "  1.0422554e-03 -5.2812113e-04  1.1073545e-03  3.8252121e-03\n",
            "  1.7368911e-05 -1.1975683e-03  1.1085335e-03 -2.6561604e-03\n",
            "  1.7511704e-03  1.5600476e-03  1.0129297e-04 -1.5142235e-03\n",
            "  2.1837926e-03 -1.0145071e-03 -3.2407136e-04 -1.5356560e-03\n",
            "  2.2687227e-03  1.0350860e-03 -1.5248937e-03  4.1752926e-04\n",
            " -2.0816368e-03  3.0446326e-04 -1.0792757e-03 -8.1170409e-04\n",
            "  1.7346171e-04 -1.7376229e-03  2.1847050e-05 -3.3287874e-03\n",
            " -2.2461155e-04 -2.6970839e-03 -7.0315070e-04  1.4014165e-03\n",
            "  2.1857312e-03 -9.1831741e-04 -1.0655707e-03 -1.5302598e-03\n",
            "  8.0620166e-04 -9.7637654e-05 -2.6768597e-04  3.5593091e-03\n",
            "  9.1479061e-04  1.0728493e-03 -3.5824275e-03 -2.5431442e-03\n",
            "  2.2008324e-03  1.1709551e-03 -2.7435098e-04 -6.5022713e-04\n",
            " -7.0168194e-04 -8.5083599e-04  1.2171308e-03  1.0786760e-03\n",
            "  2.4776151e-03  4.7946291e-04 -1.7258365e-04  7.8029645e-04\n",
            " -1.6137775e-03  1.3306831e-03 -4.9047451e-04  8.5708295e-04\n",
            "  1.2956070e-03  2.2737400e-03 -1.5837518e-03  2.5777055e-03\n",
            "  8.6913427e-04 -1.3425014e-03  2.3424795e-03  7.8021976e-06\n",
            "  2.2603741e-03 -4.4606999e-04  2.0004809e-03 -8.4939483e-04\n",
            "  3.1151355e-03  1.9965807e-03  2.5866675e-04 -1.1018149e-03\n",
            " -5.1639234e-03  4.1280623e-04  4.5048411e-04 -6.0183433e-04\n",
            " -9.7067823e-04  1.5842982e-03  2.3422933e-03 -8.2841510e-04\n",
            " -6.4761036e-05 -2.8890092e-05  2.6793187e-03  5.5409834e-04\n",
            "  7.5061282e-04  2.8537994e-03  9.5573277e-04  1.2447258e-03\n",
            " -1.5166365e-03  1.5042351e-03 -5.0312327e-04  1.5980582e-03]\n",
            "[-5.42077294e-04  1.58834097e-03  8.79154773e-04  2.87272758e-03\n",
            "  6.92019064e-04 -1.80334330e-03  1.68063119e-03  3.18323006e-03\n",
            " -3.74041591e-03 -2.17669294e-03  2.66698911e-03 -1.09529949e-03\n",
            "  1.22978340e-03  6.38302648e-04  1.89404120e-03  2.81629036e-04\n",
            "  2.85569788e-03  3.86927626e-04 -2.75335391e-03 -5.28924167e-03\n",
            "  7.53882341e-05 -7.07358820e-04  4.77016158e-03 -2.49801739e-03\n",
            " -2.52227677e-04  1.03985614e-04 -1.23452838e-03  2.65207863e-03\n",
            " -1.76611403e-03  5.05866250e-04  2.14604870e-03 -9.33738484e-04\n",
            "  1.20851351e-03 -3.16508202e-04 -5.64097776e-04  1.61191460e-03\n",
            "  1.07880379e-03  6.31103103e-05 -1.94449839e-03 -2.05978620e-04\n",
            "  5.81934815e-04 -2.07049059e-04 -2.06717919e-03  1.23858918e-03\n",
            "  1.49686181e-03  2.77758227e-04 -4.32246539e-04 -8.66398404e-05\n",
            " -1.36830160e-04  1.08961191e-03  3.03706838e-05 -1.07412855e-03\n",
            " -1.14335096e-03 -1.15036231e-03 -1.67382474e-03 -1.80153246e-03\n",
            " -1.25110580e-03 -6.23509113e-04  3.24172055e-04 -5.38426975e-04\n",
            " -8.60852248e-04 -7.33059482e-04  3.16511234e-03 -4.45978338e-04\n",
            " -3.23022297e-03  2.69280211e-03 -2.01781571e-04  2.25021364e-03\n",
            " -2.81107449e-03  1.37094161e-04  3.49134643e-05  3.48070986e-03\n",
            "  6.48418500e-04  2.45559774e-03  1.84509816e-04  2.56236148e-04\n",
            "  9.87250241e-04  4.06565116e-04 -6.26621826e-04 -1.88288267e-03\n",
            " -1.51969120e-03 -1.61022137e-04 -1.03123610e-04  1.83603831e-03\n",
            " -6.66439359e-04 -2.30468414e-03  1.35754561e-03 -1.74630387e-03\n",
            "  1.59854768e-03  6.02036191e-04 -7.72409199e-04  2.73236976e-04\n",
            "  3.69656394e-04  2.53349514e-04  4.16769227e-03  2.46485524e-05\n",
            "  1.43305550e-03 -1.57880550e-03  1.30452111e-03 -8.94893950e-04]\n",
            "[-2.2182784e-03  2.4178040e-03 -1.6172788e-03 -1.5211693e-03\n",
            "  8.9748188e-05 -2.3801313e-03  2.2331635e-04  1.9489584e-03\n",
            "  2.9035917e-04 -1.9357271e-03  3.8589387e-05 -1.0254672e-03\n",
            "  3.6531125e-04  5.1897834e-04  1.8026734e-03  1.1748396e-03\n",
            "  2.1476601e-03  1.8349692e-03  4.4786622e-04 -1.4939475e-03\n",
            " -4.6563707e-04 -3.8920080e-05  4.9415510e-04 -1.3976039e-03\n",
            "  2.4223749e-03 -2.9129177e-04 -3.2619853e-04  5.9448177e-04\n",
            " -6.5947499e-04 -1.0459307e-03  1.4809362e-03 -6.8484549e-04\n",
            "  2.9112498e-04 -3.5715378e-03 -1.9866611e-04  1.6179037e-03\n",
            "  2.1573890e-03  8.9492928e-04 -1.2801239e-03  8.7182026e-04\n",
            "  2.5277512e-04  6.4448977e-04 -1.3230393e-03 -2.6341644e-03\n",
            " -4.2812695e-04 -1.9870156e-03 -3.9370352e-04  1.8280200e-03\n",
            "  9.9747798e-05  8.4085349e-04  1.2709292e-03 -1.1994696e-03\n",
            " -2.4489202e-03  1.5799517e-03 -1.3429431e-03  1.0810017e-03\n",
            "  3.6131155e-03 -1.8764350e-03 -6.9970229e-05  3.5846228e-04\n",
            "  6.0131034e-04 -8.2730531e-04  2.3080490e-03  1.0964347e-03\n",
            " -3.3577788e-05  2.2814807e-03  5.6987952e-05  3.5182462e-04\n",
            " -5.5102515e-04  1.8027729e-03  1.9505274e-03  9.4935461e-04\n",
            "  7.1597047e-04  1.7430575e-03  2.6292168e-03  7.9436693e-05\n",
            "  4.9339124e-04 -1.0674668e-03 -4.3501443e-04 -5.1591836e-04\n",
            " -1.5134112e-03 -4.4013874e-04  5.3043658e-04 -2.2273780e-04\n",
            "  2.8615355e-04 -8.5372786e-04  2.7043142e-04  6.4219936e-04\n",
            " -1.1232961e-04  3.8937709e-04  1.7688250e-03  1.7190217e-03\n",
            " -5.4059015e-04 -6.0458202e-04  1.5868084e-03  9.3948224e-04\n",
            "  5.1955500e-04 -1.1366318e-03 -1.4745499e-03  3.3752600e-05]\n",
            "[-2.9664317e-05 -1.5102362e-03 -2.8994749e-04 -9.6220069e-04\n",
            " -3.1057557e-03 -2.6059323e-03 -1.1366552e-03  1.4344028e-03\n",
            " -2.3704596e-04 -2.6617087e-03  2.0582978e-04  1.1795487e-04\n",
            " -3.7877676e-03  3.4341500e-03 -1.2364791e-03 -1.5658687e-03\n",
            " -3.5284404e-04  1.6781110e-03  2.3367656e-04  2.0615824e-03\n",
            "  2.5242330e-03  1.4067786e-03  3.1281393e-03 -1.8951537e-03\n",
            "  2.7585817e-03  3.7401330e-04  4.4281367e-04 -6.0086686e-04\n",
            " -2.0851889e-03  2.1643206e-03  5.4898363e-04 -1.1104292e-03\n",
            "  1.8864460e-03 -4.4606896e-03 -7.0224909e-05  1.9725570e-03\n",
            " -1.0451130e-03  6.5463828e-05 -1.2592203e-04 -1.3323181e-03\n",
            " -2.5562495e-03  1.5617767e-04 -8.7134412e-04 -1.6614031e-03\n",
            "  6.6920661e-04 -3.0580920e-04  1.3155893e-03 -2.1124282e-03\n",
            "  1.4071215e-03  5.7354366e-04 -1.3576509e-04  9.8884024e-04\n",
            " -9.8458887e-04 -9.0776087e-04  3.5684351e-03  2.0081042e-04\n",
            " -9.9681108e-04  4.2177737e-05 -2.8486338e-03 -2.1143719e-03\n",
            " -1.7708413e-03  1.9778269e-04 -1.5407609e-03 -1.8309438e-04\n",
            " -7.0501713e-04  3.2604907e-03 -6.7674217e-04 -1.7215915e-03\n",
            " -1.9029059e-03  2.6933693e-03  3.1635840e-04 -4.1678725e-04\n",
            "  1.6376941e-03 -5.7847699e-04  1.6671934e-03  5.1162060e-04\n",
            " -1.9466340e-03  6.5531908e-04 -7.5298309e-04  1.3321600e-03\n",
            " -1.6434668e-03  4.3016038e-04 -1.1901972e-04  9.8046451e-04\n",
            "  1.2323868e-03 -5.2061334e-04  3.1151050e-03 -2.9032049e-04\n",
            "  1.1765598e-04  1.3876571e-03 -6.0114666e-04 -2.0158396e-03\n",
            "  2.7973212e-03 -3.3235482e-03 -1.4332912e-03  2.3634986e-03\n",
            " -3.2431395e-03 -3.9169489e-04 -3.2327275e-03  7.1915897e-04]\n",
            "[-1.6104344e-03 -1.4663419e-03  2.2837789e-04  1.7715232e-03\n",
            "  1.6829158e-03 -5.3619861e-04  1.5385039e-03  1.6342980e-03\n",
            " -1.5600993e-04 -6.3149194e-04  2.2563593e-04 -2.8545202e-03\n",
            "  7.2318071e-04  2.4318385e-03 -1.2694914e-03 -8.2966336e-04\n",
            "  3.8675577e-03 -4.5132337e-04 -1.2865686e-03 -3.3712967e-03\n",
            "  3.5084446e-04 -6.7956129e-04  2.2439531e-03 -1.1685661e-03\n",
            "  2.5378356e-03 -9.8871510e-04 -2.1860576e-03  3.7286268e-03\n",
            " -1.9558636e-03  2.1623878e-03  2.9811042e-03 -4.3919244e-05\n",
            "  1.5141570e-03 -3.0636964e-03 -4.7805626e-04 -1.5205571e-03\n",
            "  1.5746163e-03 -8.1947114e-04  2.6798346e-03  1.6191050e-03\n",
            " -1.9946818e-03 -2.1458829e-03 -1.3580208e-03 -1.2931998e-03\n",
            "  1.5479013e-04 -3.9575010e-04  1.2195049e-03  5.6597692e-05\n",
            "  1.1466104e-03  1.8922064e-03  1.2627540e-03 -1.2540775e-03\n",
            " -2.8161964e-04  7.4475101e-04  1.1865175e-04  6.9100130e-04\n",
            "  3.1346178e-03  1.9079596e-03 -1.1799465e-03  4.7609499e-03\n",
            " -8.3198305e-04 -2.6744842e-03  1.1768974e-03 -7.4237156e-05\n",
            " -2.1147644e-03  6.5425900e-04  5.9654273e-04 -5.2056625e-04\n",
            " -1.2466415e-03  3.7317725e-03  9.0987637e-04 -4.0747391e-04\n",
            "  2.9638123e-03 -7.6312071e-04  7.3350930e-05 -2.8557593e-03\n",
            " -1.2362312e-04  4.7543488e-04 -3.5628115e-04 -7.4137759e-04\n",
            " -1.9275399e-03 -3.1764733e-04 -1.7503029e-03  2.5696422e-03\n",
            " -6.8448955e-04 -1.1701656e-03 -3.1947534e-04 -1.7754261e-03\n",
            "  2.7337058e-03  1.3206624e-03  1.0389950e-03  3.5636697e-04\n",
            "  1.7209494e-03 -3.7946142e-03  2.0624851e-03  1.7435526e-04\n",
            "  2.1176038e-03 -1.5189813e-03  8.7727813e-06 -4.4432955e-04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "texts = [\n",
        "    \"El perro corre felizmente por el parque todas las mañanas.\",\n",
        "    \"Ayer estudié para el examen de matemáticas durante tres horas.\",\n",
        "    \"La lluvia cesó justo a tiempo para que pudiéramos salir de casa.\",\n",
        "    \"Ellos prefieren leer libros antes de dormir cada noche.\",\n",
        "    \"María compró un ramo de flores para decorar la sala de estar.\"\n",
        "]\n",
        "\n",
        "# 1. Tokenizar las oraciones\n",
        "tokenized_texts = [word_tokenize(text) for text in texts]\n",
        "print(tokenized_texts)\n",
        "\n",
        "# 2. Entrenar el modelo Word2Vec\n",
        "sentences = tokenized_texts\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# 3. Calcular el vector para cada oración (promedio de vectores de palabras)\n",
        "def get_sentence_vector(sentence, model):\n",
        "    \"\"\"Calcula el vector de una oración promediando los vectores de sus palabras.\"\"\"\n",
        "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if vectors:\n",
        "        return sum(vectors) / len(vectors)\n",
        "    else:\n",
        "        return [0] * model.vector_size  # Vector de ceros si no hay palabras en el vocabulario\n",
        "\n",
        "sentence_vectors = [get_sentence_vector(sentence, model) for sentence in sentences]\n",
        "\n",
        "# 4. Imprimir los vectores de las oraciones\n",
        "for vector in sentence_vectors:\n",
        "    print(vector)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ed1eb8-da00-4932-e6f4-6884433f67ea",
        "id": "0sNMcCU6-34l"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['El', 'perro', 'corre', 'felizmente', 'por', 'el', 'parque', 'todas', 'las', 'mañanas', '.'], ['Ayer', 'estudié', 'para', 'el', 'examen', 'de', 'matemáticas', 'durante', 'tres', 'horas', '.'], ['La', 'lluvia', 'cesó', 'justo', 'a', 'tiempo', 'para', 'que', 'pudiéramos', 'salir', 'de', 'casa', '.'], ['Ellos', 'prefieren', 'leer', 'libros', 'antes', 'de', 'dormir', 'cada', 'noche', '.'], ['María', 'compró', 'un', 'ramo', 'de', 'flores', 'para', 'decorar', 'la', 'sala', 'de', 'estar', '.']]\n",
            "[['El', 'perro', 'corre', 'felizmente', 'por', 'el', 'parque', 'todas', 'las', 'mañanas', '.'], ['Ayer', 'estudié', 'para', 'el', 'examen', 'de', 'matemáticas', 'durante', 'tres', 'horas', '.'], ['La', 'lluvia', 'cesó', 'justo', 'a', 'tiempo', 'para', 'que', 'pudiéramos', 'salir', 'de', 'casa', '.'], ['Ellos', 'prefieren', 'leer', 'libros', 'antes', 'de', 'dormir', 'cada', 'noche', '.'], ['María', 'compró', 'un', 'ramo', 'de', 'flores', 'para', 'decorar', 'la', 'sala', 'de', 'estar', '.']]\n",
            "[ 7.2593760e-04  2.5326270e-03  8.6171250e-04  2.7890371e-03\n",
            "  1.0422554e-03 -5.2812113e-04  1.1073545e-03  3.8252121e-03\n",
            "  1.7368911e-05 -1.1975683e-03  1.1085335e-03 -2.6561604e-03\n",
            "  1.7511704e-03  1.5600476e-03  1.0129297e-04 -1.5142235e-03\n",
            "  2.1837926e-03 -1.0145071e-03 -3.2407136e-04 -1.5356560e-03\n",
            "  2.2687227e-03  1.0350860e-03 -1.5248937e-03  4.1752926e-04\n",
            " -2.0816368e-03  3.0446326e-04 -1.0792757e-03 -8.1170409e-04\n",
            "  1.7346171e-04 -1.7376229e-03  2.1847050e-05 -3.3287874e-03\n",
            " -2.2461155e-04 -2.6970839e-03 -7.0315070e-04  1.4014165e-03\n",
            "  2.1857312e-03 -9.1831741e-04 -1.0655707e-03 -1.5302598e-03\n",
            "  8.0620166e-04 -9.7637654e-05 -2.6768597e-04  3.5593091e-03\n",
            "  9.1479061e-04  1.0728493e-03 -3.5824275e-03 -2.5431442e-03\n",
            "  2.2008324e-03  1.1709551e-03 -2.7435098e-04 -6.5022713e-04\n",
            " -7.0168194e-04 -8.5083599e-04  1.2171308e-03  1.0786760e-03\n",
            "  2.4776151e-03  4.7946291e-04 -1.7258365e-04  7.8029645e-04\n",
            " -1.6137775e-03  1.3306831e-03 -4.9047451e-04  8.5708295e-04\n",
            "  1.2956070e-03  2.2737400e-03 -1.5837518e-03  2.5777055e-03\n",
            "  8.6913427e-04 -1.3425014e-03  2.3424795e-03  7.8021976e-06\n",
            "  2.2603741e-03 -4.4606999e-04  2.0004809e-03 -8.4939483e-04\n",
            "  3.1151355e-03  1.9965807e-03  2.5866675e-04 -1.1018149e-03\n",
            " -5.1639234e-03  4.1280623e-04  4.5048411e-04 -6.0183433e-04\n",
            " -9.7067823e-04  1.5842982e-03  2.3422933e-03 -8.2841510e-04\n",
            " -6.4761036e-05 -2.8890092e-05  2.6793187e-03  5.5409834e-04\n",
            "  7.5061282e-04  2.8537994e-03  9.5573277e-04  1.2447258e-03\n",
            " -1.5166365e-03  1.5042351e-03 -5.0312327e-04  1.5980582e-03]\n",
            "[-5.42077294e-04  1.58834097e-03  8.79154773e-04  2.87272758e-03\n",
            "  6.92019064e-04 -1.80334330e-03  1.68063119e-03  3.18323006e-03\n",
            " -3.74041591e-03 -2.17669294e-03  2.66698911e-03 -1.09529949e-03\n",
            "  1.22978340e-03  6.38302648e-04  1.89404120e-03  2.81629036e-04\n",
            "  2.85569788e-03  3.86927626e-04 -2.75335391e-03 -5.28924167e-03\n",
            "  7.53882341e-05 -7.07358820e-04  4.77016158e-03 -2.49801739e-03\n",
            " -2.52227677e-04  1.03985614e-04 -1.23452838e-03  2.65207863e-03\n",
            " -1.76611403e-03  5.05866250e-04  2.14604870e-03 -9.33738484e-04\n",
            "  1.20851351e-03 -3.16508202e-04 -5.64097776e-04  1.61191460e-03\n",
            "  1.07880379e-03  6.31103103e-05 -1.94449839e-03 -2.05978620e-04\n",
            "  5.81934815e-04 -2.07049059e-04 -2.06717919e-03  1.23858918e-03\n",
            "  1.49686181e-03  2.77758227e-04 -4.32246539e-04 -8.66398404e-05\n",
            " -1.36830160e-04  1.08961191e-03  3.03706838e-05 -1.07412855e-03\n",
            " -1.14335096e-03 -1.15036231e-03 -1.67382474e-03 -1.80153246e-03\n",
            " -1.25110580e-03 -6.23509113e-04  3.24172055e-04 -5.38426975e-04\n",
            " -8.60852248e-04 -7.33059482e-04  3.16511234e-03 -4.45978338e-04\n",
            " -3.23022297e-03  2.69280211e-03 -2.01781571e-04  2.25021364e-03\n",
            " -2.81107449e-03  1.37094161e-04  3.49134643e-05  3.48070986e-03\n",
            "  6.48418500e-04  2.45559774e-03  1.84509816e-04  2.56236148e-04\n",
            "  9.87250241e-04  4.06565116e-04 -6.26621826e-04 -1.88288267e-03\n",
            " -1.51969120e-03 -1.61022137e-04 -1.03123610e-04  1.83603831e-03\n",
            " -6.66439359e-04 -2.30468414e-03  1.35754561e-03 -1.74630387e-03\n",
            "  1.59854768e-03  6.02036191e-04 -7.72409199e-04  2.73236976e-04\n",
            "  3.69656394e-04  2.53349514e-04  4.16769227e-03  2.46485524e-05\n",
            "  1.43305550e-03 -1.57880550e-03  1.30452111e-03 -8.94893950e-04]\n",
            "[-2.2182784e-03  2.4178040e-03 -1.6172788e-03 -1.5211693e-03\n",
            "  8.9748188e-05 -2.3801313e-03  2.2331635e-04  1.9489584e-03\n",
            "  2.9035917e-04 -1.9357271e-03  3.8589387e-05 -1.0254672e-03\n",
            "  3.6531125e-04  5.1897834e-04  1.8026734e-03  1.1748396e-03\n",
            "  2.1476601e-03  1.8349692e-03  4.4786622e-04 -1.4939475e-03\n",
            " -4.6563707e-04 -3.8920080e-05  4.9415510e-04 -1.3976039e-03\n",
            "  2.4223749e-03 -2.9129177e-04 -3.2619853e-04  5.9448177e-04\n",
            " -6.5947499e-04 -1.0459307e-03  1.4809362e-03 -6.8484549e-04\n",
            "  2.9112498e-04 -3.5715378e-03 -1.9866611e-04  1.6179037e-03\n",
            "  2.1573890e-03  8.9492928e-04 -1.2801239e-03  8.7182026e-04\n",
            "  2.5277512e-04  6.4448977e-04 -1.3230393e-03 -2.6341644e-03\n",
            " -4.2812695e-04 -1.9870156e-03 -3.9370352e-04  1.8280200e-03\n",
            "  9.9747798e-05  8.4085349e-04  1.2709292e-03 -1.1994696e-03\n",
            " -2.4489202e-03  1.5799517e-03 -1.3429431e-03  1.0810017e-03\n",
            "  3.6131155e-03 -1.8764350e-03 -6.9970229e-05  3.5846228e-04\n",
            "  6.0131034e-04 -8.2730531e-04  2.3080490e-03  1.0964347e-03\n",
            " -3.3577788e-05  2.2814807e-03  5.6987952e-05  3.5182462e-04\n",
            " -5.5102515e-04  1.8027729e-03  1.9505274e-03  9.4935461e-04\n",
            "  7.1597047e-04  1.7430575e-03  2.6292168e-03  7.9436693e-05\n",
            "  4.9339124e-04 -1.0674668e-03 -4.3501443e-04 -5.1591836e-04\n",
            " -1.5134112e-03 -4.4013874e-04  5.3043658e-04 -2.2273780e-04\n",
            "  2.8615355e-04 -8.5372786e-04  2.7043142e-04  6.4219936e-04\n",
            " -1.1232961e-04  3.8937709e-04  1.7688250e-03  1.7190217e-03\n",
            " -5.4059015e-04 -6.0458202e-04  1.5868084e-03  9.3948224e-04\n",
            "  5.1955500e-04 -1.1366318e-03 -1.4745499e-03  3.3752600e-05]\n",
            "[-2.9664317e-05 -1.5102362e-03 -2.8994749e-04 -9.6220069e-04\n",
            " -3.1057557e-03 -2.6059323e-03 -1.1366552e-03  1.4344028e-03\n",
            " -2.3704596e-04 -2.6617087e-03  2.0582978e-04  1.1795487e-04\n",
            " -3.7877676e-03  3.4341500e-03 -1.2364791e-03 -1.5658687e-03\n",
            " -3.5284404e-04  1.6781110e-03  2.3367656e-04  2.0615824e-03\n",
            "  2.5242330e-03  1.4067786e-03  3.1281393e-03 -1.8951537e-03\n",
            "  2.7585817e-03  3.7401330e-04  4.4281367e-04 -6.0086686e-04\n",
            " -2.0851889e-03  2.1643206e-03  5.4898363e-04 -1.1104292e-03\n",
            "  1.8864460e-03 -4.4606896e-03 -7.0224909e-05  1.9725570e-03\n",
            " -1.0451130e-03  6.5463828e-05 -1.2592203e-04 -1.3323181e-03\n",
            " -2.5562495e-03  1.5617767e-04 -8.7134412e-04 -1.6614031e-03\n",
            "  6.6920661e-04 -3.0580920e-04  1.3155893e-03 -2.1124282e-03\n",
            "  1.4071215e-03  5.7354366e-04 -1.3576509e-04  9.8884024e-04\n",
            " -9.8458887e-04 -9.0776087e-04  3.5684351e-03  2.0081042e-04\n",
            " -9.9681108e-04  4.2177737e-05 -2.8486338e-03 -2.1143719e-03\n",
            " -1.7708413e-03  1.9778269e-04 -1.5407609e-03 -1.8309438e-04\n",
            " -7.0501713e-04  3.2604907e-03 -6.7674217e-04 -1.7215915e-03\n",
            " -1.9029059e-03  2.6933693e-03  3.1635840e-04 -4.1678725e-04\n",
            "  1.6376941e-03 -5.7847699e-04  1.6671934e-03  5.1162060e-04\n",
            " -1.9466340e-03  6.5531908e-04 -7.5298309e-04  1.3321600e-03\n",
            " -1.6434668e-03  4.3016038e-04 -1.1901972e-04  9.8046451e-04\n",
            "  1.2323868e-03 -5.2061334e-04  3.1151050e-03 -2.9032049e-04\n",
            "  1.1765598e-04  1.3876571e-03 -6.0114666e-04 -2.0158396e-03\n",
            "  2.7973212e-03 -3.3235482e-03 -1.4332912e-03  2.3634986e-03\n",
            " -3.2431395e-03 -3.9169489e-04 -3.2327275e-03  7.1915897e-04]\n",
            "[-1.6104344e-03 -1.4663419e-03  2.2837789e-04  1.7715232e-03\n",
            "  1.6829158e-03 -5.3619861e-04  1.5385039e-03  1.6342980e-03\n",
            " -1.5600993e-04 -6.3149194e-04  2.2563593e-04 -2.8545202e-03\n",
            "  7.2318071e-04  2.4318385e-03 -1.2694914e-03 -8.2966336e-04\n",
            "  3.8675577e-03 -4.5132337e-04 -1.2865686e-03 -3.3712967e-03\n",
            "  3.5084446e-04 -6.7956129e-04  2.2439531e-03 -1.1685661e-03\n",
            "  2.5378356e-03 -9.8871510e-04 -2.1860576e-03  3.7286268e-03\n",
            " -1.9558636e-03  2.1623878e-03  2.9811042e-03 -4.3919244e-05\n",
            "  1.5141570e-03 -3.0636964e-03 -4.7805626e-04 -1.5205571e-03\n",
            "  1.5746163e-03 -8.1947114e-04  2.6798346e-03  1.6191050e-03\n",
            " -1.9946818e-03 -2.1458829e-03 -1.3580208e-03 -1.2931998e-03\n",
            "  1.5479013e-04 -3.9575010e-04  1.2195049e-03  5.6597692e-05\n",
            "  1.1466104e-03  1.8922064e-03  1.2627540e-03 -1.2540775e-03\n",
            " -2.8161964e-04  7.4475101e-04  1.1865175e-04  6.9100130e-04\n",
            "  3.1346178e-03  1.9079596e-03 -1.1799465e-03  4.7609499e-03\n",
            " -8.3198305e-04 -2.6744842e-03  1.1768974e-03 -7.4237156e-05\n",
            " -2.1147644e-03  6.5425900e-04  5.9654273e-04 -5.2056625e-04\n",
            " -1.2466415e-03  3.7317725e-03  9.0987637e-04 -4.0747391e-04\n",
            "  2.9638123e-03 -7.6312071e-04  7.3350930e-05 -2.8557593e-03\n",
            " -1.2362312e-04  4.7543488e-04 -3.5628115e-04 -7.4137759e-04\n",
            " -1.9275399e-03 -3.1764733e-04 -1.7503029e-03  2.5696422e-03\n",
            " -6.8448955e-04 -1.1701656e-03 -3.1947534e-04 -1.7754261e-03\n",
            "  2.7337058e-03  1.3206624e-03  1.0389950e-03  3.5636697e-04\n",
            "  1.7209494e-03 -3.7946142e-03  2.0624851e-03  1.7435526e-04\n",
            "  2.1176038e-03 -1.5189813e-03  8.7727813e-06 -4.4432955e-04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clasificación de textos con Naive Bayes**\n",
        "Naive Bayes es un algoritmo común para la clasificación de texto. Se basa en la teoría de probabilidad de Bayes y es muy eficiente para la clasificación de correos electrónicos (spam vs no spam), análisis de sentimientos, etc."
      ],
      "metadata": {
        "id": "WyOh4BSQF4wi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPJNFP2T2OiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "\n",
        "corpus = ['I love this movie', 'This is a terrible movie' , 'This is a good movie']\n",
        "labels = [1, 0, 1]  # 1: positivo, 0: negativo\n",
        "vectorizer = CountVectorizer()\n"
      ],
      "metadata": {
        "id": "QqcHMeOBF-xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = vectorizer.fit_transform(corpus)\n",
        "model = MultinomialNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "test = ['This movie is amazing']\n",
        "test_vector = vectorizer.transform(test)\n",
        "\n",
        "prediction = model.predict(test_vector)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(prediction)\n",
        "\n",
        "px = np.dot(X.toarray(), test_vector.toarray().T).reshape(-1)\n",
        "print(np.argmax(px))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsABJcUP2Phk",
        "outputId": "c769eb6d-062f-42eb-bb3f-f7b7f911bb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['good' 'is' 'love' 'movie' 'terrible' 'this']\n",
            "[1]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN y LSTM para procesamiento secuencial**\n",
        "Las Redes Neuronales Recurrentes (RNN) y los modelos LSTM (Long Short-Term Memory) son útiles para procesar secuencias de texto, ya que pueden capturar dependencias a largo plazo en los datos."
      ],
      "metadata": {
        "id": "yiXDH5pcGBxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vWlkUAq5moA",
        "outputId": "77cc08af-389d-4a35-b9e5-ac68c8236204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "\n",
        "sentences = [\"I love NLP\", \"NLP is amazing\"]\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "sentences = [\"I love NLP\", \"NLP is amazing\"]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=1000, output_dim=64))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "KXLfP1JVGI_5",
        "outputId": "92dfd9db-3bf5-4528-e4fb-87a1b0e1b2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-6b1d526e283a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}